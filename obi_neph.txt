#!/bin/bash
#SBATCH -J 3.neph.obi
#SBATCH -N 1
#SBATCH --ntasks-per-node 28
#SBATCH -o %N.%J.out # Output file (node.jobid.out)
#SBATCH -e %N.%J.err # Error file (node.jobid.err)
#SBATCH -p compute
#SBATCH --exclusive


####    Dependencies    ####
##echo install cdbfasta
##git clone https://github.com/gpertea/cdbfasta.git
##cd cdbfasta/
##make


module load obitools/1.2.9 
module load R/3.3.0
module load crop/1.33/gcc-6.3.0
vsearch=~/applications/vsearch-2.5.0/bin/vsearch
fastqc=~/applications/FastQC/fastqc
export PATH=$PATH:~/applications/cdbfasta/
export PATH=$PATH:~/applications/seqtk/
date


####################    preprocessing raw data for obitools pipeline    ####################

#echo sometimes raw data can already be demultiplexed but for the obitools pipeline you need samples concatenated into single R1 and R2 reads. This
#echo is a preprocessing pipeline to get the data ready for analysis using Obitools. Process raw data, assuming data is in folder "reads_raw" 
#echo in .gz format, e.g. neph-MiSeq_S1_L001_R1_001.fastq.gz and neph-MiSeq_S1_L001_R2_001.fastq.gz

#echo 1. folder with demultiplexed reads, e.g. CASAVA from MiSeq, unzip and save gzipped reads to reads_gzipd
#mkdir reads_raw_gzipd
#for i in $(ls reads_raw/*.gz| cut -d "." -f 1); do gunzip -c reads_raw/${i##*/}.fastq.gz > reads_raw_zipd/${i##*/}.fastq; done
 
#echo 2. trim primers from R1 and R2 reads, the value of the -u parameter is the primer length. If primer length is different then
#echo run separately for R1 and R2
#mkdir reads_raw_gzipd_trimmed
#for i in $(ls reads_raw_gzipd/*.fastq | cut -d "." -f 1,2); do cutadapt -u 28 -o reads_raw_gzipd_trimmed/${i##*/}.trimmed.fastq reads_raw_gzipd/${i##*/}.fastq; done ; gzip reads_raw_gzipd_trimmed/*.fastq # note data have been gzipped for the next command

#echo 3. stitch new adaptor and primer sequence with high base quality "I" to fastq file. You need 2 files, e.g. R1_list.txt and R2_list.txt, which
#echo contain the "path_to_file/file_name.fastq,adaptor_sequence,primer_sequence". One line per sample. CREDIT: CHRISTOPH HAHN 
#mkdir raw_reads_obi
#echo R1
#for line in $(cat list_R1.csv); do file=$(echo $line| cut -d "," -f 1); zcat <(echo "$line" | gzip) $file | perl -ne 'if ($.==1){chomp; @a=split(","); $bc=$a[1]; $primer=$a[2]; $qualdummy='I' x (length($bc)+length($primer));}else{$h=$_; $s=<>; $p=<>; $q=<>; print "$h$bc$primer$s$p$qualdummy$q"}'; done | gzip > raw_reads_obi/concat_R1.fastq.gz

#echo R2
#for line in $(cat list_R2.csv); do file=$(echo $line| cut -d "," -f 1); zcat <(echo "$line" | gzip) $file | perl -ne 'if ($.==1){chomp; @a=split(","); $bc=$a[1]; $primer=$a[2]; $qualdummy='I' x (length($bc)+length($primer));}else{$h=$_; $s=<>; $p=<>; $q=<>; print "$h$bc$primer$s$p$qualdummy$q"}'; done | gzip > raw_reads_obi/concat_R2.fastq.gz

#echo preprocessing complete, data now ready to be run through obitools pipeline using concat_R1.fastq.gz and concat_R2.fastq.gz files


####################    quality control obitools input data    ####################

#echo QC of obitools input data
#echo 1. gunzip data for QC

#mkdir obitools
#cd obitools
#gunzip -c ../raw_reads_obi/*.gz > .

#echo 2. fastqc all read R1 and R2
#mkdir fastqc
#$fastqc -o fastqc/ --extract -f fastq *.fastq
#echo check for length quality, trim accordingly

#echo trimming
#obicut -e 250 concat_R1.fastq > neph_trim250.R1.fastq
#obicut -e 180 concat_R2.fastq > neph_trim180.R2.fastq


####################    obitools    ####################

echo Paired-end alignment. Annotate the reads with quality 40 and split the output in two files
illuminapairedend -r neph_trim180.R2.fastq neph_trim250.R1.fastq | obiannotate -S goodali:'"Good_nephCOI" if score>40.00 else "Bad_nephCOI"' | obisplit -t goodali

#echo convert fastq to fasta for demultiplexing in parallel
#$seqtk seq -a Good_nephCOI.fastq > Good_nephCOI.fasta

###########

#echo Demultiplexing with ngsfilter
#echo here use the script "submit_parallel_ngsfilter.sh" to parallelize the ngsfilter command, you need to edit the paths to your Good_nephCOI.fastq
#echo  generated fastq file. This script generates files and folders by splitting the Good_nephCOI.fastq file into 1000 sequences per file and 100
#echo files per folder. It will generate  as many files/folders necessary. You can set the split files into hatever you like and it will run each file
#echo as a separate batch jobs. Typically 1000 sequences per file run in about 2 minutes 40 seconds.

#mkdir demulti
#cd demulti

#sh submit_parallel_ngsfilter.sh #usage ngsfilter -t ngsfilter_neph2011.txt --fasta-output -u unidentified_nephCOI.fastq Good_nephCOI.fastq --DEBUG > neph.filtered.fasta

#ngsfilter -t ngsfilter_nephfull.txt --fasta-output -u unidentified_nephCOI.fastq Good_nephCOI.fasta --DEBUG > neph.filtered.fasta

#echo Once ngsfilter has complete, e.g. 1-2 hours, concatenate all *.filtered.fasta from all folders
#ngsfilter_results=~/Stanford_Jan2018/full_neph/demulti
#cat $(find $ngsfilter_results -name '*.filtered.fasta' | xargs)> demulti/neph.filtered.fasta
#cat $(find $ngsfilter_results -name '*unidentified*' | xargs)> demulti/unidentified.neph.fasta
###### check reads number ######

#echo sort neph.filtered.fasta
#grep ">" demulti/neph.filtered.fasta | sed 's/>//g' | sort -k1.6n > demulti/neph.filtered_idlist.txt
#cdbfasta demulti/neph.filtered.fasta -o demulti/neph.filtered.fasta.index
#cat demulti/neph.filtered_idlist.txt | cdbyank demulti/neph.filtered.fasta.index > demulti/neph.filtered_sorted.fasta
#rm demulti/neph.filtered.fasta.index


###########

#echo Filter the seqs with length between 300 and 320 bp and with no 'N'
#obigrep -p 'seq_length>300' -p 'seq_length<320' -s '^[ACGT]+$' demulti/neph.filtered_sorted.fasta > neph.filtered_length.fasta

#echo Calculate stats per sample
#obistat -c sample -a seq_length neph.filtered_length.fasta > sample_stats_neph.length_filter.txt

#echo Group the unique seqs
#obiuniq -m sample neph.filtered_length.fasta > neph.unique.fasta

#echo Exchange the identifier to a short index
#obiannotate --seq-rank neph.unique.fasta | obiannotate --set-identifier '"'neph'_%09d" % seq_rank' > neph.new.fasta
#Rscript ~/peter/applications/R_scripts_metabarpark/owi_obifasta2vsearch -i neph.new.fasta -o neph.vsearch.fasta
#sed 's/ ;/;/g' neph.vsearch.fasta > neph.vsearch.mod.fasta


#############################
##### CHIMERA DETECTION #####
#############################

#echo Run UCHIME de novo in VSEARCH
#mkdir vsearch
#$vsearch --uchime_denovo neph.vsearch.mod.fasta --sizeout --nonchimeras vsearch/neph.nonchimeras.fasta --chimeras vsearch/neph.chimeras.fasta --threads 28 --uchimeout vsearch/neph.uchimeout2.txt &> vsearch/log.neph_chimeras
#sed 's/;/ ;/g' vsearch/neph.nonchimeras.fasta |grep -e ">" | awk 'sub(/^>/, "")' | awk '{print $1}' > vsearch/neph.nonchimeras.txt # text file used for owi_recount_sumaclust step


#####################
##### CLUSTERING ####
#####################

#echo swarm using vsearch nonchimeras file
#mkdir swarm
#~/peter/applications/swarm/src/swarm -d 13 -z -t 40 -o swarm/neph_SWARM13_output -s swarm/neph_SWARM13_stats -w swarm/neph_SWARM13_seeds.fasta vsearch/neph.nonchimeras.fasta



################################
##### TAXONOMIC ASSIGNMENT #####
################################

#mkdir ecotag
#cd ecotag
#echo here use the script "submit_parallel_ecotag.sh" to parallelize the ecotag command, you need to edit the paths to you sumaclust generated 
#echo fasta file, e.g. neph.sumaclust95.centers.fasta, and ecopcr database. This script generates files and folders by splitting the 
#echo neph.sumaclust95.centers.fasta file into 100 sequences per file and 100 files per folder. It will generate  as many files/folders. 
#echo You can set the split files into hatever you like and it will run each file as a separate job. Typically 100 sequneces per file run in about ~10-15 minutes.

#sh submit_parallel_ecotag.sh

######################

#echo Once the previous step has complete, e.g. overnight, concatenate all *.ecotag.fasta from all folders
#ecotag_results=~/stanford/neph_final/ecotag_all/
#cat $(find $ecotag_results -name '*.ecotag.fasta' | xargs)> ecotag_all/neph.ecotag.fasta

#echo To sort fasta file numerically
#echo install cdbfasta
#git clone https://github.com/gpertea/cdbfasta.git
#cd cdbfasta/
#make
#echo sort ecotag.fasta  
#grep ">" ecotag_all/neph.ecotag.fasta | sed 's/>//g' | sort -k1.6n > ecotag_all/neph.ecotag_idlist.txt
#cdbfasta ecotag_all/neph.ecotag.fasta -o ecotag_all/neph.ecotag.fasta.index
#cat ecotag_all/neph.ecotag_idlist.txt | cdbyank ecotag_all/neph.ecotag.fasta.index > ecotag_all/neph.ecotag_sorted.fasta
#rm ecotag_all/neph.ecotag.fasta.index

######################
## R scripts for reformatting metabarcoding databases CREDIT: OWEN WANGENSTEEN Find R scripts here: https://github.com/metabarpark/R_scripts_metabarpark
#echo Add taxa above order level
#Rscript ~/peter/applications/R_scripts_metabarpark/owi_add_taxonomy ecotag_all/neph.ecotag_sorted.fasta neph.ecotag.fasta.annotated.csv

#echo recount abundance by sample
#obitab -o neph.new.fasta > neph.new.tab
#Rscript ~/peter/applications/R_scripts_metabarpark/owi_recount_swarm swarm/neph_SWARM13_output neph.new.tab

#echo combine ecotag and abundance files
#Rscript ~/peter/applications/R_scripts_metabarpark/owi_combine -i neph.ecotag.fasta.annotated.csv -a swarm/neph_SWARM13_output.counts.csv -o neph_all_SWARM_FINAL_MOTUs.csv

#echo collapse MOTUs
#Rscript ~/peter/applications/R_scripts_metabarpark/owi_collapse -s 14 -e 106 -i neph_all_SWARM_FINAL_MOTUs.csv


date

module purge